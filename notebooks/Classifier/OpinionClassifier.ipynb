{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                    2000 ‡∂ë‡∂ö‡∑ô‡∂ö‡∑ä‡∂Ø üòÇ\n",
      "1                              ‡∂∏‡∑ú‡∂© ‡∂¥‡∑ú‡∂Ω‡∑í‡∑É‡∑ä gothyeküòÇ\n",
      "2                                    ‡∑Ñ‡∑ú‡∂Ø ‡∑Ä‡∑ê‡∂©‡∑ö..üòÇüòÇüòÇ\n",
      "3                       ‡∂ö‡∑ú‡∂Ω‡∑ä‡∂Ω‡∂ß bike ‡∂ë‡∂ö‡∂ß ‡∂Ü‡∑É ‡∑Ñ‡∑í‡∂≠‡∑í‡∂ΩüòÖüòÖ\n",
      "4                                         ‡∂ú‡∑Ñ‡∂¥‡∑í‡∂∫ üòÇüíî\n",
      "                          ...                     \n",
      "495                            ‡∂∂‡∂∫‡∑í‡∂ö‡∑ä aka hambunada\n",
      "496                        ‡∂å‡∂ß ‡∂¥‡∂Ø‡∑í‡∂±‡∑ä‡∂± ‡∂Ü‡∑É ‡∑Ñ‡∑í‡∂≠‡∑í‡∂Ω‡∑è ü•¥üòÇüëç\n",
      "497                          ‡∂ú‡∑Ñ‡∂¥‡∂±‡∑ä ‡∂Ö‡∂∫‡∑í‡∂∫‡∑ô ‡∂Ö‡∂Ω‡∑ä‡∂Ω‡∂ú‡∑ô‡∂±üòÇüòÇ\n",
      "498     ‡∂ö‡∂±‡∂¥‡∂Ω‡∑è ‡∂ú‡∂±‡∑ä‡∂±‡∑ö ‡∂±‡∑ê‡∂≠‡∑î‡∑Ä ‡∂∂‡∑ê‡∑Ñ‡∑í‡∂±‡∑ä‡∂± ‡∂ö‡∑í‡∑Ä‡∑ä‡∑Ä‡∑è ‡∂±‡∂∏‡∑ä ‡∑Ñ‡∂ª‡∑í üòÖ\n",
      "499    @Sanju D Liyanage ‡∂ö‡∑ú‡∑Ñ‡∑ô ‡∂ú‡∑í‡∂∫‡∂≠‡∑ä ‡∂¥‡∑ú‡∂Ω‡∑í‡∑É‡∑ä ‡∂¥‡∂ö‡∑ô‡∂ö‡∑ä üòÖ\n",
      "Name: Comment, Length: 500, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../../data/replaced_english_to_sinhala.csv')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "# Display up to 500 rows from the \"Comment\" column\n",
    "comments_subset = df['Comment'].head(500)\n",
    "print(comments_subset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment Id</th>\n",
       "      <th>User Id</th>\n",
       "      <th>Reply Comment Id</th>\n",
       "      <th>Digg Count</th>\n",
       "      <th>Reply Count</th>\n",
       "      <th>Video Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.970000e+03</td>\n",
       "      <td>3.970000e+03</td>\n",
       "      <td>3.970000e+03</td>\n",
       "      <td>3970.000000</td>\n",
       "      <td>3904.000000</td>\n",
       "      <td>3.970000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.221770e+18</td>\n",
       "      <td>6.953478e+18</td>\n",
       "      <td>1.199558e+17</td>\n",
       "      <td>1.136524</td>\n",
       "      <td>0.074283</td>\n",
       "      <td>7.210500e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.123808e+16</td>\n",
       "      <td>4.107298e+17</td>\n",
       "      <td>9.226974e+17</td>\n",
       "      <td>22.970278</td>\n",
       "      <td>1.103933</td>\n",
       "      <td>1.024129e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>7.210438e+18</td>\n",
       "      <td>1.690231e+15</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.210500e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.210718e+18</td>\n",
       "      <td>6.849306e+18</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.210500e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.211414e+18</td>\n",
       "      <td>6.998108e+18</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.210500e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.230551e+18</td>\n",
       "      <td>7.119436e+18</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.210500e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.331623e+18</td>\n",
       "      <td>7.321315e+18</td>\n",
       "      <td>7.238263e+18</td>\n",
       "      <td>1075.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>7.210500e+18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Comment Id       User Id  Reply Comment Id   Digg Count  Reply Count  \\\n",
       "count  3.970000e+03  3.970000e+03      3.970000e+03  3970.000000  3904.000000   \n",
       "mean   7.221770e+18  6.953478e+18      1.199558e+17     1.136524     0.074283   \n",
       "std    2.123808e+16  4.107298e+17      9.226974e+17    22.970278     1.103933   \n",
       "min    7.210438e+18  1.690231e+15      0.000000e+00     0.000000     0.000000   \n",
       "25%    7.210718e+18  6.849306e+18      0.000000e+00     0.000000     0.000000   \n",
       "50%    7.211414e+18  6.998108e+18      0.000000e+00     0.000000     0.000000   \n",
       "75%    7.230551e+18  7.119436e+18      0.000000e+00     0.000000     0.000000   \n",
       "max    7.331623e+18  7.321315e+18      7.238263e+18  1075.000000    47.000000   \n",
       "\n",
       "           Video Id  \n",
       "count  3.970000e+03  \n",
       "mean   7.210500e+18  \n",
       "std    1.024129e+03  \n",
       "min    7.210500e+18  \n",
       "25%    7.210500e+18  \n",
       "50%    7.210500e+18  \n",
       "75%    7.210500e+18  \n",
       "max    7.210500e+18  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Comment Id            0\n",
       "User Id               0\n",
       "Unique Id             0\n",
       "Nickname              1\n",
       "Avatar                0\n",
       "User URL              0\n",
       "Reply Comment Id      0\n",
       "Comment               0\n",
       "Digg Count            0\n",
       "Reply Count          66\n",
       "Is Author Digged      0\n",
       "Author Pin           66\n",
       "Language            420\n",
       "Create Time           0\n",
       "Video URL             0\n",
       "Video Id              0\n",
       "English_Words         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to C:\\Users\\Zenith\n",
      "[nltk_data]     Anthony\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sinhala Video Type: Opinion\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "\n",
    "def analyze_sinhala_comments(comments):\n",
    "    # Initialize the sentiment analyzer\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # Define keywords associated with claims and opinions in Sinhala\n",
    "    claim_keywords_sinhala = ['‡∂≠‡∑ì‡∂ª‡∂´‡∂∫', '‡∑É‡∑è‡∂ö‡∂†‡∑ä‡∂°‡∑è‡∑Ä', '‡∂¥‡∑ä‚Äç‡∂ª‡∂≠‡∑ä‚Äç‡∂∫‡∑è‡∂±‡∂∫']\n",
    "    opinion_keywords_sinhala = ['‡∂Ö‡∂Ø‡∑Ñ‡∑É‡∑ä', '‡∂Ö‡∂±‡∑è‡∑Ä‡∑ê‡∂ö‡∑ä‡∑Ç‡∑è‡∑Ä', '‡∂ö‡∑í‡∑Ä‡∑ä‡∑Ä']\n",
    "\n",
    "    # Initialize counts for claim and opinion keywords\n",
    "    claim_keyword_count = 0\n",
    "    opinion_keyword_count = 0\n",
    "\n",
    "    # Initialize sentiments list\n",
    "    sentiments = []\n",
    "\n",
    "    # Analyze sentiment and count keyword occurrences\n",
    "    for comment in comments:\n",
    "        sentiment_score = sid.polarity_scores(comment)['compound']\n",
    "\n",
    "        # Classify comments as positive, negative, or neutral based on compound score\n",
    "        if sentiment_score >= 0.05:\n",
    "            sentiment = 'positive'\n",
    "        elif sentiment_score <= -0.05:\n",
    "            sentiment = 'negative'\n",
    "        else:\n",
    "            sentiment = 'neutral'\n",
    "\n",
    "        sentiments.append(sentiment)\n",
    "\n",
    "        # Count occurrences of claim and opinion keywords in comments\n",
    "        claim_keyword_count += sum(1 for keyword in claim_keywords_sinhala if re.search(rf'\\b{keyword}\\b', comment, re.IGNORECASE))\n",
    "        opinion_keyword_count += sum(1 for keyword in opinion_keywords_sinhala if re.search(rf'\\b{keyword}\\b', comment, re.IGNORECASE))\n",
    "\n",
    "    # Make a determination based on sentiment and keyword analysis\n",
    "    overall_sentiment = max(set(sentiments), key=sentiments.count)\n",
    "    if overall_sentiment == 'positive' and opinion_keyword_count > claim_keyword_count:\n",
    "        return 'Opinion'\n",
    "    elif overall_sentiment == 'negative' and opinion_keyword_count > claim_keyword_count:\n",
    "        return 'Opinion'\n",
    "    elif overall_sentiment == 'neutral' and opinion_keyword_count > claim_keyword_count:\n",
    "        return 'Opinion'\n",
    "    else:\n",
    "        return 'Claim'\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('../../data/replaced_english_to_sinhala.csv')\n",
    "\n",
    "# Assuming 'Comment' is the column containing Sinhala comments\n",
    "sinhala_comments = df['Comment']\n",
    "\n",
    "video_type = analyze_sinhala_comments(sinhala_comments)\n",
    "print(f'Sinhala Video Type: {video_type}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e974f9c24b004fa1ad97cb9fb1fcc5ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Zenith Anthony\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Zenith Anthony\\.cache\\huggingface\\hub\\models--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "At least one of TensorFlow 2.0 or PyTorch should be installed. To install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ To install PyTorch, read the instructions at https://pytorch.org/.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load pre-trained sentiment analysis model\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m sentiment_analysis \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentiment-analysis\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Assuming 'Comment' is the column containing Sinhala comments\u001b[39;00m\n\u001b[0;32m      8\u001b[0m comments \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mComment\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Zenith Anthony\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:870\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    869\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m--> 870\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m    881\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[1;32mc:\\Users\\Zenith Anthony\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\pipelines\\base.py:229\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;124;03mSelect framework (TensorFlow or PyTorch) to use from the `model` passed. Returns a tuple (framework, model).\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;124;03m    `Tuple`: A tuple framework, model.\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tf_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_available():\n\u001b[1;32m--> 229\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    230\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one of TensorFlow 2.0 or PyTorch should be installed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    231\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    232\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo install PyTorch, read the instructions at https://pytorch.org/.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    233\u001b[0m     )\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    235\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_pipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m task\n",
      "\u001b[1;31mRuntimeError\u001b[0m: At least one of TensorFlow 2.0 or PyTorch should be installed. To install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ To install PyTorch, read the instructions at https://pytorch.org/."
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import pandas as pd\n",
    "\n",
    "# Load pre-trained sentiment analysis model\n",
    "sentiment_analysis = pipeline('sentiment-analysis')\n",
    "\n",
    "# Assuming 'Comment' is the column containing Sinhala comments\n",
    "comments = df['Comment']\n",
    "\n",
    "# Perform sentiment analysis on each comment\n",
    "sentiments = sentiment_analysis(comments.tolist())\n",
    "\n",
    "# Add the sentiment labels to the DataFrame\n",
    "df['Sentiment_Label'] = [sentiment['label'] for sentiment in sentiments]\n",
    "df['Sentiment_Score'] = [sentiment['score'] for sentiment in sentiments]\n",
    "\n",
    "# Define keywords associated with claims and opinions in Sinhala\n",
    "claim_keywords_sinhala = ['‡∂≠‡∑ì‡∂ª‡∂´‡∂∫', '‡∑É‡∑è‡∂ö‡∂†‡∑ä‡∂°‡∑è‡∑Ä', '‡∂¥‡∑ä‚Äç‡∂ª‡∂≠‡∑ä‚Äç‡∂∫‡∑è‡∂±‡∂∫']\n",
    "opinion_keywords_sinhala = ['‡∂Ö‡∂Ø‡∑Ñ‡∑É‡∑ä', '‡∂Ö‡∂±‡∑è‡∑Ä‡∑ê‡∂ö‡∑ä‡∑Ç‡∑è‡∑Ä', '‡∂ö‡∑í‡∑Ä‡∑ä‡∑Ä']\n",
    "\n",
    "# Count occurrences of claim and opinion keywords in comments\n",
    "df['Claim_Keyword_Count'] = df['Comment'].apply(lambda x: sum(1 for keyword in claim_keywords_sinhala if re.search(rf'\\b{keyword}\\b', x, re.IGNORECASE)))\n",
    "df['Opinion_Keyword_Count'] = df['Comment'].apply(lambda x: sum(1 for keyword in opinion_keywords_sinhala if re.search(rf'\\b{keyword}\\b', x, re.IGNORECASE)))\n",
    "\n",
    "# Make a determination based on sentiment and keyword analysis\n",
    "df['Video_Type'] = 'Claim'\n",
    "df.loc[(df['Sentiment_Label'] == 'POSITIVE') & (df['Opinion_Keyword_Count'] > df['Claim_Keyword_Count']), 'Video_Type'] = 'Opinion'\n",
    "df.loc[(df['Sentiment_Label'] == 'NEGATIVE') & (df['Opinion_Keyword_Count'] > df['Claim_Keyword_Count']), 'Video_Type'] = 'Opinion'\n",
    "df.loc[(df['Sentiment_Label'] == 'NEUTRAL') & (df['Opinion_Keyword_Count'] > df['Claim_Keyword_Count']), 'Video_Type'] = 'Opinion'\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(df[['Comment', 'Sentiment_Label', 'Sentiment_Score', 'Claim_Keyword_Count', 'Opinion_Keyword_Count', 'Video_Type']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfeade9c106d415797242e9f70f03179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/669M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Zenith Anthony\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Zenith Anthony\\.cache\\huggingface\\hub\\models--nlptown--bert-base-multilingual-uncased-sentiment. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Sinhala Sentiment: positive\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n",
    "# Load your CSV file\n",
    "df = pd.read_csv('../../data/replaced_english_to_sinhala.csv')\n",
    "\n",
    "# Assuming 'Comment' is the column containing Sinhala comments\n",
    "comments = df['Comment']\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer for sentiment analysis\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize and predict sentiment for each comment\n",
    "sentiments = []\n",
    "for comment in comments:\n",
    "    inputs = tokenizer(comment, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_class = logits.argmax().item()\n",
    "    sentiments.append(predicted_class)\n",
    "\n",
    "# Determine overall sentiment\n",
    "positive_count = sum(1 for sentiment in sentiments if sentiment == 2)  # BERT sentiment scale: 0-negative, 1-neutral, 2-positive\n",
    "negative_count = sum(1 for sentiment in sentiments if sentiment == 0)\n",
    "\n",
    "# Make a determination based on overall sentiment\n",
    "if positive_count > negative_count:\n",
    "    overall_sentiment = 'positive'\n",
    "elif positive_count < negative_count:\n",
    "    overall_sentiment = 'negative'\n",
    "else:\n",
    "    overall_sentiment = 'neutral'\n",
    "\n",
    "# Output the result\n",
    "print(f'Overall Sinhala Sentiment: {overall_sentiment}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
