{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b93b8ce1-9ccb-43ce-8fa8-a063323225ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langid.langid import LanguageIdentifier, model\n",
    "import pandas as pd\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caf3cbce-91b6-4b7e-a5df-8dbf5609ead7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\UOM\\\\L4S1\\\\Research\\\\Implementation\\\\FYP-Research\\\\notebooks'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d5cab92-154a-4c91-9f30-03f18beaf15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Comment Id              User Id        Unique Id  \\\n",
      "0  7314765604681810706  6876105532748530693  20dark_hunter00   \n",
      "1  7314907545561907970  6982416764534309894        kavi___09   \n",
      "2  7314935455757484801  7187578911432705029    romeshjanaka7   \n",
      "3  7314731362967257857  6698376036462527493            u2me_   \n",
      "4  7314854241218118408  6854570108179678214    asunthapathum   \n",
      "\n",
      "           Nickname                                             Avatar  \\\n",
      "0       Dark Hunter  https://p16-sign-useast2a.tiktokcdn.com/tos-us...   \n",
      "1  Kavindu tharusha  https://p16-sign-sg.tiktokcdn.com/aweme/100x10...   \n",
      "2     romeshjanaka7  https://p16-sign-useast2a.tiktokcdn.com/tos-us...   \n",
      "3              U2me  https://p16-sign-va.tiktokcdn.com/musically-ma...   \n",
      "4    Asuntha Pathum  https://p16-sign-sg.tiktokcdn.com/aweme/100x10...   \n",
      "\n",
      "                                  User URL  Reply Comment Id  \\\n",
      "0  https://www.tiktok.com/@20dark_hunter00                 0   \n",
      "1        https://www.tiktok.com/@kavi___09                 0   \n",
      "2    https://www.tiktok.com/@romeshjanaka7                 0   \n",
      "3            https://www.tiktok.com/@u2me_                 0   \n",
      "4    https://www.tiktok.com/@asunthapathum                 0   \n",
      "\n",
      "                                             Comment  Digg Count  Reply Count  \\\n",
      "0  ‡∂á‡∂∫‡∑í display issues ‡∂ú‡∑ê‡∂± ‡∂ö‡∂≠‡∑è ‡∂ö‡∂ª‡∂±‡∑ä‡∂±‡∑ö ‡∂±‡∑ê‡∂≠‡∑ä‡∂≠‡∑ö S ser...          22         46.0   \n",
      "1                        Update ‡∂ö‡∂ª‡∑è‡∂ß ‡∂¥‡∑É‡∑ä‡∑É‡∑ö ‡∂â‡∂ª‡∑í ‡∂∫‡∂∫‡∑í‡∂ØüëÄ          16         12.0   \n",
      "2                              ‡∂¥‡∂≠ ‡∂Ω‡∂∫‡∑í‡∂±‡∑ä ‡∂ë‡∂ö‡∂ö‡∑ä ‡∂ú‡∑í‡∂∫‡∑è‡∂∏ üòÇ          10          9.0   \n",
      "3                   üòÇ ‡∑Ä‡∑ê‡∂©‡∂ö‡∑ä ‡∑Ä‡∑ô‡∂±‡∑ä ‡∂±‡∑ë ‡∂∂‡∂±‡∑ä ‡∂ú‡∂±‡∑ä‡∂± ‡∑Ä‡∑ô‡∂±‡∑ä ‡∂±‡∑ë          28          2.0   \n",
      "4                                 gerrn line enwne üòÇ           5          4.0   \n",
      "\n",
      "   Is Author Digged  Author Pin Language          Create Time  \\\n",
      "0              True         0.0       si  2023-12-21 01:16:57   \n",
      "1              True         0.0       si  2023-12-21 10:27:32   \n",
      "2              True         0.0       si  2023-12-21 12:15:55   \n",
      "3              True         0.0       si  2023-12-20 23:03:58   \n",
      "4              True         0.0       en  2023-12-21 07:00:55   \n",
      "\n",
      "                                           Video URL             Video Id  \n",
      "0  https://www.tiktok.com/@tech_with_wicky/video/...  7314721689806376197  \n",
      "1  https://www.tiktok.com/@tech_with_wicky/video/...  7314721689806376197  \n",
      "2  https://www.tiktok.com/@tech_with_wicky/video/...  7314721689806376197  \n",
      "3  https://www.tiktok.com/@tech_with_wicky/video/...  7314721689806376197  \n",
      "4  https://www.tiktok.com/@tech_with_wicky/video/...  7314721689806376197  \n"
     ]
    }
   ],
   "source": [
    "import chardet\n",
    "\n",
    "# Detect encoding\n",
    "with open(\"../data/dataset.xlsx\", 'rb') as f:\n",
    "    result = chardet.detect(f.read())\n",
    "    encoding = result['encoding']\n",
    "\n",
    "# Read the file with the detected encoding\n",
    "df = pd.read_excel(\"../data/dataset.xlsx\")\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c24516b6-8ef3-431e-ae14-80bca511c5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Comment Language\n",
      "0   ‡∂á‡∂∫‡∑í display issues ‡∂ú‡∑ê‡∂± ‡∂ö‡∂≠‡∑è ‡∂ö‡∂ª‡∂±‡∑ä‡∂±‡∑ö ‡∂±‡∑ê‡∂≠‡∑ä‡∂≠‡∑ö S ser...       si\n",
      "1                         Update ‡∂ö‡∂ª‡∑è‡∂ß ‡∂¥‡∑É‡∑ä‡∑É‡∑ö ‡∂â‡∂ª‡∑í ‡∂∫‡∂∫‡∑í‡∂ØüëÄ       si\n",
      "2                               ‡∂¥‡∂≠ ‡∂Ω‡∂∫‡∑í‡∂±‡∑ä ‡∂ë‡∂ö‡∂ö‡∑ä ‡∂ú‡∑í‡∂∫‡∑è‡∂∏ üòÇ       si\n",
      "3                    üòÇ ‡∑Ä‡∑ê‡∂©‡∂ö‡∑ä ‡∑Ä‡∑ô‡∂±‡∑ä ‡∂±‡∑ë ‡∂∂‡∂±‡∑ä ‡∂ú‡∂±‡∑ä‡∂± ‡∑Ä‡∑ô‡∂±‡∑ä ‡∂±‡∑ë       si\n",
      "5                          ‡∂á‡∂¥‡∂Ω‡∑ä ‡∑Ä‡∂Ω‡∂ß‡∂≠‡∑ä ‡∂ö‡∑ô‡∂Ω‡∑í‡∂∫ ‡∂ë‡∑Ñ‡∑ô‡∂±‡∂∏‡∑ä üòÇüòÇ       si\n",
      "6                   ‡∂Ö‡∂ª ‡∑Ä‡∑ô‡∂±‡∑É‡∑ä ‡∂ö‡∂ª‡∂Ω‡∑è ‡∂ö‡∑ù‡∂Ω‡∑ä ‡∂ú‡∂±‡∑ä‡∂± ‡∂ë‡∂ö ‡∑É‡∑ô‡∂ß‡∑ä ‡∂á       si\n",
      "7                                   ‡∂ö‡∑í‡∂∫‡∂ö‡∑ä ‡∑Ä‡∑í‡∂≠‡∂ª ‡∑Ä‡∑ö‡∑Ä‡∑í‡∂Ø?       si\n",
      "11                            Samsumg ‡∂≠‡∂∏‡∑è ‡∂ë‡∂Ø‡∂≠‡∑ä ‡∂Ö‡∂Ø‡∂≠‡∑äüñ§üî•       si\n",
      "12                        pixel ‡∑Ä‡∂Ω‡∂ß‡∂≠‡∑ä Review ‡∂ë‡∂ö‡∂ö‡∑ä ‡∂ï‡∂±‡∑ô       si\n",
      "15                                  ‡∂ë‡∑Ñ‡∑ô‡∂±‡∂∏‡∑ä ‡∂ë‡∂ö‡∂ö‡∑ä ‡∂ú‡∂∏‡∑î üôÇ       si\n",
      "16                                      ‡∑É‡∑ê‡∂∏‡∑ä‡∑É‡∑î‡∂±‡∑ä‡∂ú‡∑ä ü§ç‚ú®       si\n",
      "18          ‡∂ß‡∂∫‡∑í‡∂ß‡∑ö‡∂±‡∑í‡∂∫‡∂∏‡∑ä ‡∂ö‡∂±‡∑ä‡∂±‡∂Ø ‡∂ö‡∑í‡∂∫‡∂¥‡∑î ‡∑É‡∑ê‡∂∏‡∑ä‡∑É‡∂±‡∑ä ‡∑Ü‡∑ë‡∂±‡∑ä ‡∂Ω‡∑è..üòÖ       si\n",
      "19                         s23 eka ‡∂∏‡∑ö ‡∂∏‡∑è‡∑É‡∑ô ‡∂ú‡∂≠‡∑ä‡∂≠ ‡∂∏‡∂∏ ü•∫üòì       si\n",
      "20                                              üòÇ ‡∂â‡∂∫‡∑è       si\n",
      "22                               ‡∑Ñ‡∑ú‡∂Ø‡∂∏ ‡∂ë‡∂ö ‡∂Ω‡∑ù‡∂ö‡∑ô ‡∂Ø‡∑ê‡∂±‡∑ä‡∂±‡∂∏‡∑ñ       si\n",
      "23                    ‡∂≠‡∑è‡∂∏ m02 ‡∂ë‡∂ö‡∂ö‡∑ä ‡∂¥‡∑è‡∑Ä‡∑í‡∂†‡∑ä‡∂†‡∑í ‡∂ö‡∂ª‡∂± ‡∂∏‡∂∏ üôÇüëç       si\n",
      "24                  Mata ahuneme ‡∂í ‡∂á‡∂∫‡∑í kiyala maüòÖüòÖüòÖüòÖüòÖ       si\n",
      "25  Apple ‡∑Ä‡∂Ω ‡∂ß‡∂∫‡∑í‡∂ß‡∑ö‡∂±‡∑í‡∂∫‡∂∏‡∑ä ‡∑Ä‡∂Ω‡∂ß ‡∂∂‡∑ê‡∂± ‡∂∂‡∑ê‡∂± ‡∑Ñ‡∑í‡∂ß‡∂¥‡∑î ‡∂ã‡∂±‡∑ä ‡∂ß‡∂∫‡∑í‡∂ß...       si\n",
      "26  ‡∂Ω‡∂Ç‡∂ö‡∑è‡∑Ä‡∑ö ‡∂Ö‡∂¥‡∑í‡∂ß ‡∂∏‡∑ú‡∂ö‡∂ö‡∑ä ‡∂Ü‡∑Ä‡∂≠‡∑ä ‡∂ú‡∂±‡∑ä‡∂± ‡∑Ä‡∑ô‡∂±‡∑ä‡∂±‡∑ë 25000‡∂ß ‡∂Ü‡∂¥‡∑î ...       si\n",
      "29  @Nirosh Madushan ‡∂ß‡∑ô‡∂Ω‡∑í‡∑É‡∑ä‡∂ö‡∑ú‡∂¥‡∑ä ‡∂ë‡∂ö‡∂ö‡∑ä ‡∂ë‡∂±‡∑ä‡∂±‡∑ê‡∂Ω‡∑î ‡∂∂‡∂±‡∑ä 1...       si\n",
      "45                       ‡∂¥‡∂Ω‡∂≠‡∑î‡∂ª‡∑î ‡∑Ä‡∂Ω‡∂ß ‡∑Ä‡∂©‡∑è ‡∂±‡∂∏‡∑ä ‡∑Ñ‡∑ú‡∂Ø‡∂∫‡∑í üíÄ‚úåÔ∏è       si\n",
      "47  s23 294 dan $$ 257 ‡∑Ä‡∑ê‡∂© ‡∑É‡∂¥‡∑í‡∂ª‡∑í ipone ‡∂ö‡∑ê‡∂Ω‡∑ö ‡∂ë‡∂≠‡∂ª‡∂±‡∑ä ...       si\n",
      "48  ‡∂∂‡∑ú‡∂ª‡∑î‡∑Ä‡∂ß green line ‡∂ú‡∑ê‡∂± ‡∂ö‡∂≠‡∑è ‡∂ö‡∂ª‡∂±‡∑ä‡∂± ‡∂ë‡∂¥‡∑è ‡∑Ñ‡∑ê‡∂∏ ‡∂ë‡∂ö‡∂ß‡∂∏ ‡∂ë...       si\n",
      "49  green lineüòÇ\\n‡∂ß‡∂∫‡∑í‡∂ß‡∑ö‡∂±‡∑í‡∂∫‡∂∏‡∑ä ‡∑Ä‡∂Ω‡∂ß ‡∂∂‡∑ê‡∂±‡∑ä‡∂± ‡∂ã‡∂±‡∑ä ‡∂∏‡∑Ñ‡∑í‡∂±‡∑ä‡∂Ø ‡∂ë...       si\n",
      "50                              ‡∂á‡∂¥‡∂Ω‡∑ä ‡∂ö‡∑ú‡∑Ñ‡∑ô‡∂Ø ‡∑Ñ‡∑î‡∂≠‡∑ä‡∂≠‡∑ù ‡∑Ñ‡∑ú‡∂≥       si\n",
      "53  ‡∂ª‡∑ê‡∂Ω‡∑ä‡∂Ω‡∂ß ‡∂±‡∂∏‡∑ä ‡∂±‡∑ô‡∂∏‡∑ö ‡∑Ñ‡∑ê‡∂∂‡∑ê‡∂∫‡∑í ‡∂Ö‡∑Ä‡∑î‡∂ª‡∑î‡∂Ø‡∑î 3 4 k ‡∂ã‡∂±‡∂≠‡∑ä spee...       si\n",
      "54  ‡∂∏‡∂∏ apple samsung ‡∑Ä‡∂Ω flagship phone ‡∂¥‡∑è‡∑Ä‡∑í‡∂†‡∑ä‡∂†‡∑í ‡∂ö‡∂ª...       si\n",
      "64  green line ‡∑Ä‡∑í‡∂≠‡∂ª‡∂∫‡∑í ‡∂±‡∑ö. iphone ‡∑Ä‡∂ú‡∑ö ‡∂∏‡∑î‡∑Ö‡∑î screen ‡∂ë...       si\n",
      "69                         ‡∂≠‡∑è‡∂∏ ‡∂±‡∑ë ‡∂≠‡∑Ä ‡∂Ö‡∑Ä‡∑î‡∂ª‡∑î‡∂Ø‡∑ä‡∂Ø‡∂ö‡∑í‡∂±‡∑ä ‡∂ë‡∂∫‡∑í       si\n",
      "91  ane hutta ‡∂≠‡∂∏‡∂∫‡∑í ‡∂Ö‡∑Ä‡∑î‡∂ª‡∑î‡∂Ø‡∑î ganak s serios use ‡∂ö‡∂ª‡∂±‡∑ä...       si\n",
      "92  ‡∂∏‡∑è‡∂≠‡∑ä ‡∂Ø‡∑ê‡∂±‡∑ä ‡∂Ö‡∑Ä‡∑î‡∂ª‡∑î‡∂Ø‡∑î ‡∂ú‡∑è‡∂±‡∂ö‡∑ä ‡∂≠‡∑í‡∑É‡∑ä‡∑É‡∑ö Samsung ‡∂∫‡∑î‡∑É‡∑ä ‡∂ö‡∂ª...       si\n",
      "93  ‡∂ë‡∂±‡∑ä‡∂± ‡∂ö‡∂Ω‡∑í‡∂∏‡∑ä ‡∂≠‡∑ù ‡∂ö‡∑ú‡∑Ñ‡∑ú‡∂∏‡∂Ø ‡∂∂‡∂Ç ‡∂ö‡∑í‡∂∫‡∂±‡∑ä‡∂±‡∑ö üòÇüíî ‡∂Ü‡∑Ä‡∂∏ ‡∂ö‡∑í‡∂∫‡∂¥‡∂±‡∑ä ...       si\n",
      "96                                 ‡∂Ö‡∂¥‡∑í‡∂ß‡∂≠‡∑ä ‡∂ë‡∂±‡∑ä‡∂± ‡∂ë‡∂¥‡∑ê ?üíÄ       si\n",
      "97  wadak na ‡∂ö‡∑í‡∂∫‡∂Ω‡∑è . chaina use karana un ge padi ...       si\n",
      "98  ‡∂ö‡∑ù ‡∂∂‡∂±‡∑ä ‡∂â‡∂≠‡∑í‡∂±‡∑ä ‡∂Ü‡∑Ä‡∑ô ‡∂±‡∑ë‡∂±‡∑ô. S9 ‡∂ë‡∂ö‡∂ö‡∑î‡∂≠‡∑ä ‡∂≠‡∑í‡∂∫‡∑ö ‡∂≠‡∑è‡∂∏ ‡∂â‡∂ª‡∂ö‡∑ä...       si\n"
     ]
    }
   ],
   "source": [
    "# Function to identify the language of a comment\n",
    "def identify_language(comment):\n",
    "    lang, _ = langid.classify(comment)\n",
    "    return lang\n",
    "\n",
    "# Apply the language identification function to the \"Comment\" column\n",
    "df['Language'] = df['Comment'].apply(identify_language)\n",
    "\n",
    "# Filter only Sinhala comments\n",
    "sinhala_comments_df = df[df['Language'] == 'si']\n",
    "\n",
    "# Display the Sinhala comments DataFrame\n",
    "print(sinhala_comments_df[['Comment', 'Language']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "715e87f9-4908-4abe-b499-bdcb5f1a1410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "     ---------------------------------------- 0.0/981.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/981.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/981.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 10.2/981.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 10.2/981.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 10.2/981.5 kB ? eta -:--:--\n",
      "     - ----------------------------------- 30.7/981.5 kB 146.3 kB/s eta 0:00:07\n",
      "     - ----------------------------------- 30.7/981.5 kB 146.3 kB/s eta 0:00:07\n",
      "     - ----------------------------------- 41.0/981.5 kB 131.3 kB/s eta 0:00:08\n",
      "     -- ---------------------------------- 61.4/981.5 kB 172.4 kB/s eta 0:00:06\n",
      "     --- --------------------------------- 92.2/981.5 kB 228.2 kB/s eta 0:00:04\n",
      "     --- --------------------------------- 92.2/981.5 kB 228.2 kB/s eta 0:00:04\n",
      "     --- --------------------------------- 92.2/981.5 kB 228.2 kB/s eta 0:00:04\n",
      "     --- --------------------------------- 92.2/981.5 kB 228.2 kB/s eta 0:00:04\n",
      "     --- --------------------------------- 92.2/981.5 kB 228.2 kB/s eta 0:00:04\n",
      "     --- --------------------------------- 92.2/981.5 kB 228.2 kB/s eta 0:00:04\n",
      "     ------ ----------------------------- 174.1/981.5 kB 256.0 kB/s eta 0:00:04\n",
      "     ------- ---------------------------- 194.6/981.5 kB 281.0 kB/s eta 0:00:03\n",
      "     -------- --------------------------- 225.3/981.5 kB 299.1 kB/s eta 0:00:03\n",
      "     -------- --------------------------- 225.3/981.5 kB 299.1 kB/s eta 0:00:03\n",
      "     -------- --------------------------- 225.3/981.5 kB 299.1 kB/s eta 0:00:03\n",
      "     --------- -------------------------- 245.8/981.5 kB 274.2 kB/s eta 0:00:03\n",
      "     ---------- ------------------------- 276.5/981.5 kB 288.8 kB/s eta 0:00:03\n",
      "     ---------- ------------------------- 286.7/981.5 kB 285.3 kB/s eta 0:00:03\n",
      "     ------------ ----------------------- 337.9/981.5 kB 322.8 kB/s eta 0:00:02\n",
      "     ------------- ---------------------- 358.4/981.5 kB 327.8 kB/s eta 0:00:02\n",
      "     ------------- ---------------------- 368.6/981.5 kB 327.6 kB/s eta 0:00:02\n",
      "     --------------- -------------------- 409.6/981.5 kB 345.5 kB/s eta 0:00:02\n",
      "     ---------------- ------------------- 440.3/981.5 kB 362.3 kB/s eta 0:00:02\n",
      "     ----------------- ------------------ 471.0/981.5 kB 373.4 kB/s eta 0:00:02\n",
      "     ------------------ ----------------- 501.8/981.5 kB 383.7 kB/s eta 0:00:02\n",
      "     -------------------- --------------- 553.0/981.5 kB 403.8 kB/s eta 0:00:02\n",
      "     --------------------- -------------- 583.7/981.5 kB 417.0 kB/s eta 0:00:01\n",
      "     --------------------- -------------- 583.7/981.5 kB 417.0 kB/s eta 0:00:01\n",
      "     ------------------------ ----------- 665.6/981.5 kB 446.1 kB/s eta 0:00:01\n",
      "     ------------------------- ---------- 696.3/981.5 kB 452.9 kB/s eta 0:00:01\n",
      "     --------------------------- -------- 747.5/981.5 kB 472.0 kB/s eta 0:00:01\n",
      "     ---------------------------- ------- 778.2/981.5 kB 472.6 kB/s eta 0:00:01\n",
      "     ----------------------------- ------ 809.0/981.5 kB 487.0 kB/s eta 0:00:01\n",
      "     ----------------------------- ------ 809.0/981.5 kB 487.0 kB/s eta 0:00:01\n",
      "     ----------------------------- ------ 809.0/981.5 kB 487.0 kB/s eta 0:00:01\n",
      "     ----------------------------- ------ 809.0/981.5 kB 487.0 kB/s eta 0:00:01\n",
      "     ----------------------------- ------ 809.0/981.5 kB 487.0 kB/s eta 0:00:01\n",
      "     ----------------------------- ------ 809.0/981.5 kB 487.0 kB/s eta 0:00:01\n",
      "     ------------------------------ ----- 829.4/981.5 kB 422.9 kB/s eta 0:00:01\n",
      "     ------------------------------------ 981.5/981.5 kB 489.5 kB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: six in d:\\uom\\l4s1\\research\\implementation\\fyp-research\\env\\lib\\site-packages (from langdetect) (1.16.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993254 sha256=20172f06bc589962f8ff42787bd6830296f067f45266e0e02ce8cbe5b893ce28\n",
      "  Stored in directory: c:\\users\\asus\\appdata\\local\\pip\\cache\\wheels\\0a\\f2\\b2\\e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n"
     ]
    }
   ],
   "source": [
    "%pip install langdetect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab09b76-cad9-4be5-83ba-25fb39b420ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to remove English words from a sentence\n",
    "# def remove_english_words(sentence):\n",
    "#     # Split the sentence into words\n",
    "#     words = sentence.split()\n",
    "    \n",
    "#     # Filter out English words\n",
    "#     non_english_words = [word for word in words if detect(word) != 'en']\n",
    "    \n",
    "#     # Join the non-English words to form the filtered sentence\n",
    "#     filtered_sentence = ' '.join(non_english_words)\n",
    "    \n",
    "#     return filtered_sentence\n",
    "\n",
    "# # Apply the remove_english_words function to the \"Comment\" column\n",
    "# df['Filtered_Comment'] = df['Comment'].apply(remove_english_words)\n",
    "\n",
    "# # Display the DataFrame with the filtered comments\n",
    "# print(df[['Comment', 'Filtered_Comment']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5c1255-7422-43d7-9f65-3191bce18081",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
