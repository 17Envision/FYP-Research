{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b93b8ce1-9ccb-43ce-8fa8-a063323225ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langid.langid import LanguageIdentifier, model\n",
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caf3cbce-91b6-4b7e-a5df-8dbf5609ead7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\UOM\\\\L4S1\\\\Research\\\\Implementation\\\\FYP-Research\\\\notebooks\\\\AB_Sentiment_Analysis'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d5cab92-154a-4c91-9f30-03f18beaf15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Comment Id              User Id        Unique Id  \\\n",
      "0  7314765604681810706  6876105532748530693  20dark_hunter00   \n",
      "1  7314907545561907970  6982416764534309894        kavi___09   \n",
      "2  7314935455757484801  7187578911432705029    romeshjanaka7   \n",
      "3  7314731362967257857  6698376036462527493            u2me_   \n",
      "4  7314854241218118408  6854570108179678214    asunthapathum   \n",
      "\n",
      "           Nickname                                             Avatar  \\\n",
      "0       Dark Hunter  https://p16-sign-useast2a.tiktokcdn.com/tos-us...   \n",
      "1  Kavindu tharusha  https://p16-sign-sg.tiktokcdn.com/aweme/100x10...   \n",
      "2     romeshjanaka7  https://p16-sign-useast2a.tiktokcdn.com/tos-us...   \n",
      "3              U2me  https://p16-sign-va.tiktokcdn.com/musically-ma...   \n",
      "4    Asuntha Pathum  https://p16-sign-sg.tiktokcdn.com/aweme/100x10...   \n",
      "\n",
      "                                  User URL  Reply Comment Id  \\\n",
      "0  https://www.tiktok.com/@20dark_hunter00                 0   \n",
      "1        https://www.tiktok.com/@kavi___09                 0   \n",
      "2    https://www.tiktok.com/@romeshjanaka7                 0   \n",
      "3            https://www.tiktok.com/@u2me_                 0   \n",
      "4    https://www.tiktok.com/@asunthapathum                 0   \n",
      "\n",
      "                                             Comment  Digg Count  Reply Count  \\\n",
      "0  ‡∂á‡∂∫‡∑í display issues ‡∂ú‡∑ê‡∂± ‡∂ö‡∂≠‡∑è ‡∂ö‡∂ª‡∂±‡∑ä‡∂±‡∑ö ‡∂±‡∑ê‡∂≠‡∑ä‡∂≠‡∑ö S ser...          22         46.0   \n",
      "1                        Update ‡∂ö‡∂ª‡∑è‡∂ß ‡∂¥‡∑É‡∑ä‡∑É‡∑ö ‡∂â‡∂ª‡∑í ‡∂∫‡∂∫‡∑í‡∂ØüëÄ          16         12.0   \n",
      "2                              ‡∂¥‡∂≠ ‡∂Ω‡∂∫‡∑í‡∂±‡∑ä ‡∂ë‡∂ö‡∂ö‡∑ä ‡∂ú‡∑í‡∂∫‡∑è‡∂∏ üòÇ          10          9.0   \n",
      "3                   üòÇ ‡∑Ä‡∑ê‡∂©‡∂ö‡∑ä ‡∑Ä‡∑ô‡∂±‡∑ä ‡∂±‡∑ë ‡∂∂‡∂±‡∑ä ‡∂ú‡∂±‡∑ä‡∂± ‡∑Ä‡∑ô‡∂±‡∑ä ‡∂±‡∑ë          28          2.0   \n",
      "4                                 gerrn line enwne üòÇ           5          4.0   \n",
      "\n",
      "   Is Author Digged  Author Pin Language          Create Time  \\\n",
      "0              True         0.0       si  2023-12-21 01:16:57   \n",
      "1              True         0.0       si  2023-12-21 10:27:32   \n",
      "2              True         0.0       si  2023-12-21 12:15:55   \n",
      "3              True         0.0       si  2023-12-20 23:03:58   \n",
      "4              True         0.0       en  2023-12-21 07:00:55   \n",
      "\n",
      "                                           Video URL             Video Id  \n",
      "0  https://www.tiktok.com/@tech_with_wicky/video/...  7314721689806375936  \n",
      "1  https://www.tiktok.com/@tech_with_wicky/video/...  7314721689806375936  \n",
      "2  https://www.tiktok.com/@tech_with_wicky/video/...  7314721689806375936  \n",
      "3  https://www.tiktok.com/@tech_with_wicky/video/...  7314721689806375936  \n",
      "4  https://www.tiktok.com/@tech_with_wicky/video/...  7314721689806375936  \n"
     ]
    }
   ],
   "source": [
    "import chardet\n",
    "\n",
    "# Detect encoding\n",
    "with open(\"../../data/new_dataset.xlsx\", 'rb') as f:\n",
    "    result = chardet.detect(f.read())\n",
    "    encoding = result['encoding']\n",
    "\n",
    "# Read the file with the detected encoding\n",
    "df = pd.read_excel(\"../../data/new_dataset.xlsx\")\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c24516b6-8ef3-431e-ae14-80bca511c5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Comment Language\n",
      "0    ‡∂á‡∂∫‡∑í display issues ‡∂ú‡∑ê‡∂± ‡∂ö‡∂≠‡∑è ‡∂ö‡∂ª‡∂±‡∑ä‡∂±‡∑ö ‡∂±‡∑ê‡∂≠‡∑ä‡∂≠‡∑ö S ser...       si\n",
      "1                          Update ‡∂ö‡∂ª‡∑è‡∂ß ‡∂¥‡∑É‡∑ä‡∑É‡∑ö ‡∂â‡∂ª‡∑í ‡∂∫‡∂∫‡∑í‡∂ØüëÄ       si\n",
      "2                                ‡∂¥‡∂≠ ‡∂Ω‡∂∫‡∑í‡∂±‡∑ä ‡∂ë‡∂ö‡∂ö‡∑ä ‡∂ú‡∑í‡∂∫‡∑è‡∂∏ üòÇ       si\n",
      "3                     üòÇ ‡∑Ä‡∑ê‡∂©‡∂ö‡∑ä ‡∑Ä‡∑ô‡∂±‡∑ä ‡∂±‡∑ë ‡∂∂‡∂±‡∑ä ‡∂ú‡∂±‡∑ä‡∂± ‡∑Ä‡∑ô‡∂±‡∑ä ‡∂±‡∑ë       si\n",
      "5                           ‡∂á‡∂¥‡∂Ω‡∑ä ‡∑Ä‡∂Ω‡∂ß‡∂≠‡∑ä ‡∂ö‡∑ô‡∂Ω‡∑í‡∂∫ ‡∂ë‡∑Ñ‡∑ô‡∂±‡∂∏‡∑ä üòÇüòÇ       si\n",
      "..                                                 ...      ...\n",
      "195  ‡∂ª‡∑î‡∂∏‡∑ö‡∂±‡∑í‡∂∫‡∑è ‡∂∫‡∂±‡∑Ä‡∂±‡∂∏‡∑ä ‡∂ª‡∑ú‡∑É‡∑ô‡∂Ω‡∑ä ‡∂í‡∂¢‡∂±‡∑ä‡∑É‡∑í‡∂∫‡∑ô‡∂±‡∑ä ‡∂∫‡∂±‡∑ä‡∂±. ‡∂ö‡∑í‡∑É‡∑í ‡∂Ø...       si\n",
      "196  ‡∂ú‡∑ô‡∂Ø‡∂ª ‡∂±‡∑ô‡∑Ä‡∑ô‡∂∫‡∑í.‡∂Ω‡∂Ç‡∂ö‡∑è‡∑Ä ‡∂±‡∑ô‡∑Ä‡∑ô‡∂∫‡∑í ‡∂ª‡∂ß ‡∂¢‡∑í‡∑Ä‡∑í‡∂≠‡∑ô ‡∂ö‡∑í‡∂∫‡∂±‡∑ä‡∂±‡∑ô,‡∂Ö‡∂±‡∑í...       si\n",
      "197  ‡∂∏‡∂∏‡∂≠‡∑ä ‡∂ª‡∑î‡∂∏‡∑ö‡∂±‡∑í‡∂∫‡∑è‡∑Ä‡∑ô ‡∂≠‡∂∏‡∂∫‡∑í ‡∂â‡∂±‡∑ä‡∂±‡∑ô.‡∂∏‡∑ô‡∑Ñ‡∑ö ‡∑É‡∑ê‡∂¥ ‡∂ú‡∂±‡∑ä‡∂± ‡∂∂‡∑ë.‡∂Ö‡∂¥...       si\n",
      "198  ‡∂∏‡∂Ç ‡∂á‡∑Ä‡∑í‡∂≠‡∑ä ‡∂Ö‡∑Ä‡∂ª‡∑î‡∂Ø‡∑î 7i ‡∂∏‡∂ß‡∂±‡∂∏‡∑ä hodai ‡∂ú‡∑è‡∂∏‡∂±‡∑ä‡∂ß‡∑ä ‡∂â‡∂±‡∑ä‡∂±‡∑ö ‡∂Ü...       si\n",
      "199  ‡∂ö‡∑Ä‡∑î‡∂ª‡∑î‡∂≠‡∑ä ‡∂∏‡∑ö‡∑Ä‡∑è ‡∂ö‡∑í‡∑Ä‡∑è‡∂ß ‡∑Ä‡∑í‡∑Å‡∑ä‡∑Ä‡∑è‡∑É ‡∂ö‡∂ª‡∂±‡∑ä‡∂±‡∑ö ‡∂±‡∑ê‡∑Ñ‡∑ê ‡∂Ø‡∂±‡∑ä ‡∑Ñ‡∑ê‡∂∏...       si\n",
      "\n",
      "[90 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import langid\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have a DataFrame named 'df' with a 'Comment' column\n",
    "# ...\n",
    "\n",
    "# Function to identify the language of a comment\n",
    "def identify_language(comment):\n",
    "    lang, _ = langid.classify(comment)\n",
    "    return lang\n",
    "\n",
    "# Apply the language identification function to the \"Comment\" column\n",
    "df['Language'] = df['Comment'].apply(identify_language)\n",
    "\n",
    "# Filter only Sinhala comments\n",
    "sinhala_comments_df = df[df['Language'] == 'si']\n",
    "\n",
    "# Display the Sinhala comments DataFrame\n",
    "print(sinhala_comments_df[['Comment', 'Language']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "715e87f9-4908-4abe-b499-bdcb5f1a1410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langdetect in d:\\uom\\l4s1\\research\\implementation\\fyp-research\\env\\lib\\site-packages (1.0.9)\n",
      "Requirement already satisfied: six in d:\\uom\\l4s1\\research\\implementation\\fyp-research\\env\\lib\\site-packages (from langdetect) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langdetect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dab09b76-cad9-4be5-83ba-25fb39b420ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to remove English words from a sentence\n",
    "# def remove_english_words(sentence):\n",
    "#     # Split the sentence into words\n",
    "#     words = sentence.split()\n",
    "    \n",
    "#     # Filter out English words\n",
    "#     non_english_words = [word for word in words if detect(word) != 'en']\n",
    "    \n",
    "#     # Join the non-English words to form the filtered sentence\n",
    "#     filtered_sentence = ' '.join(non_english_words)\n",
    "    \n",
    "#     return filtered_sentence\n",
    "\n",
    "# # Apply the remove_english_words function to the \"Comment\" column\n",
    "# df['Filtered_Comment'] = df['Comment'].apply(remove_english_words)\n",
    "\n",
    "# # Display the DataFrame with the filtered comments\n",
    "# print(df[['Comment', 'Filtered_Comment']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0998aef-8af4-4230-949d-42ac483be2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡∂á‡∂∫‡∑í   ‡∂ú‡∑ê‡∂± ‡∂ö‡∂≠‡∑è ‡∂ö‡∂ª‡∂±‡∑ä‡∂±‡∑ö ‡∂±‡∑ê‡∂≠‡∑ä‡∂≠‡∑ö  ...  ‡∂ö‡∂ª‡∑è‡∂ß ‡∂¥‡∑É‡∑ä‡∑É‡∑ö ‡∂â‡∂ª‡∑í ‡∂∫‡∂∫‡∑í‡∂ØüëÄ ‡∂¥‡∂≠ ‡∂Ω‡∂∫‡∑í‡∂±‡∑ä ‡∂ë‡∂ö‡∂ö‡∑ä ‡∂ú‡∑í‡∂∫‡∑è‡∂∏ üòÇ\n"
     ]
    }
   ],
   "source": [
    "def remove_english_words(sinhala_text):\n",
    "    # Define a regular expression pattern to match English words\n",
    "    english_word_pattern = re.compile(r'\\b[a-zA-Z]+\\b')\n",
    "\n",
    "    # Use sub() to replace matched English words with an empty string\n",
    "    cleaned_text = re.sub(english_word_pattern, '', sinhala_text)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "# Example usage\n",
    "sinhala_comment = \"‡∂á‡∂∫‡∑í display issues ‡∂ú‡∑ê‡∂± ‡∂ö‡∂≠‡∑è ‡∂ö‡∂ª‡∂±‡∑ä‡∂±‡∑ö ‡∂±‡∑ê‡∂≠‡∑ä‡∂≠‡∑ö S ser... Update ‡∂ö‡∂ª‡∑è‡∂ß ‡∂¥‡∑É‡∑ä‡∑É‡∑ö ‡∂â‡∂ª‡∑í ‡∂∫‡∂∫‡∑í‡∂ØüëÄ ‡∂¥‡∂≠ ‡∂Ω‡∂∫‡∑í‡∂±‡∑ä ‡∂ë‡∂ö‡∂ö‡∑ä ‡∂ú‡∑í‡∂∫‡∑è‡∂∏ üòÇ\"\n",
    "cleaned_comment = remove_english_words(sinhala_comment)\n",
    "print(cleaned_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c186a7-ef32-40aa-8d99-18327f142b51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51a974da-f70f-426e-a37a-f81136232910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned Columns:\n",
      "                                               Comment Language\n",
      "0    ‡∂á‡∂∫‡∑í   ‡∂ú‡∑ê‡∂± ‡∂ö‡∂≠‡∑è ‡∂ö‡∂ª‡∂±‡∑ä‡∂±‡∑ö ‡∂±‡∑ê‡∂≠‡∑ä‡∂≠‡∑ö   ‡∂î‡∂ö‡∑ä‡∂ö‡∑ú‡∂ß‡∂∏ ‡∂î‡∂∫ ‡∂ö‡∑ö‡∑É‡∑ä ...         \n",
      "1                                 ‡∂ö‡∂ª‡∑è‡∂ß ‡∂¥‡∑É‡∑ä‡∑É‡∑ö ‡∂â‡∂ª‡∑í ‡∂∫‡∂∫‡∑í‡∂ØüëÄ         \n",
      "2                                ‡∂¥‡∂≠ ‡∂Ω‡∂∫‡∑í‡∂±‡∑ä ‡∂ë‡∂ö‡∂ö‡∑ä ‡∂ú‡∑í‡∂∫‡∑è‡∂∏ üòÇ         \n",
      "3                     üòÇ ‡∑Ä‡∑ê‡∂©‡∂ö‡∑ä ‡∑Ä‡∑ô‡∂±‡∑ä ‡∂±‡∑ë ‡∂∂‡∂±‡∑ä ‡∂ú‡∂±‡∑ä‡∂± ‡∑Ä‡∑ô‡∂±‡∑ä ‡∂±‡∑ë         \n",
      "5                           ‡∂á‡∂¥‡∂Ω‡∑ä ‡∑Ä‡∂Ω‡∂ß‡∂≠‡∑ä ‡∂ö‡∑ô‡∂Ω‡∑í‡∂∫ ‡∂ë‡∑Ñ‡∑ô‡∂±‡∂∏‡∑ä üòÇüòÇ         \n",
      "..                                                 ...      ...\n",
      "195  ‡∂ª‡∑î‡∂∏‡∑ö‡∂±‡∑í‡∂∫‡∑è ‡∂∫‡∂±‡∑Ä‡∂±‡∂∏‡∑ä ‡∂ª‡∑ú‡∑É‡∑ô‡∂Ω‡∑ä ‡∂í‡∂¢‡∂±‡∑ä‡∑É‡∑í‡∂∫‡∑ô‡∂±‡∑ä ‡∂∫‡∂±‡∑ä‡∂±. ‡∂ö‡∑í‡∑É‡∑í ‡∂Ø...         \n",
      "196  ‡∂ú‡∑ô‡∂Ø‡∂ª ‡∂±‡∑ô‡∑Ä‡∑ô‡∂∫‡∑í.‡∂Ω‡∂Ç‡∂ö‡∑è‡∑Ä ‡∂±‡∑ô‡∑Ä‡∑ô‡∂∫‡∑í ‡∂ª‡∂ß ‡∂¢‡∑í‡∑Ä‡∑í‡∂≠‡∑ô ‡∂ö‡∑í‡∂∫‡∂±‡∑ä‡∂±‡∑ô,‡∂Ö‡∂±‡∑í...         \n",
      "197  ‡∂∏‡∂∏‡∂≠‡∑ä ‡∂ª‡∑î‡∂∏‡∑ö‡∂±‡∑í‡∂∫‡∑è‡∑Ä‡∑ô ‡∂≠‡∂∏‡∂∫‡∑í ‡∂â‡∂±‡∑ä‡∂±‡∑ô.‡∂∏‡∑ô‡∑Ñ‡∑ö ‡∑É‡∑ê‡∂¥ ‡∂ú‡∂±‡∑ä‡∂± ‡∂∂‡∑ë.‡∂Ö‡∂¥...         \n",
      "198  ‡∂∏‡∂Ç ‡∂á‡∑Ä‡∑í‡∂≠‡∑ä ‡∂Ö‡∑Ä‡∂ª‡∑î‡∂Ø‡∑î 7i ‡∂∏‡∂ß‡∂±‡∂∏‡∑ä  ‡∂ú‡∑è‡∂∏‡∂±‡∑ä‡∂ß‡∑ä ‡∂â‡∂±‡∑ä‡∂±‡∑ö ‡∂Ü‡∑Ä‡∑ö  ‡∂ö...         \n",
      "199  ‡∂ö‡∑Ä‡∑î‡∂ª‡∑î‡∂≠‡∑ä ‡∂∏‡∑ö‡∑Ä‡∑è ‡∂ö‡∑í‡∑Ä‡∑è‡∂ß ‡∑Ä‡∑í‡∑Å‡∑ä‡∑Ä‡∑è‡∑É ‡∂ö‡∂ª‡∂±‡∑ä‡∂±‡∑ö ‡∂±‡∑ê‡∑Ñ‡∑ê ‡∂Ø‡∂±‡∑ä ‡∑Ñ‡∑ê‡∂∏...         \n",
      "\n",
      "[90 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def remove_english_words(column):\n",
    "    # Define a regular expression pattern to match English words\n",
    "    english_word_pattern = re.compile(r'\\b[a-zA-Z]+\\b')\n",
    "\n",
    "    # Use sub() to replace matched English words with an empty string\n",
    "    cleaned_column = column.apply(lambda x: re.sub(english_word_pattern, '', x))\n",
    "\n",
    "    return cleaned_column\n",
    "\n",
    "# Example usage\n",
    "# Assuming you have a DataFrame named 'sinhala_comments_df'\n",
    "# sinhala_comments_df[['Comment', 'Language']] will give you the desired columns\n",
    "columns_to_clean = sinhala_comments_df[['Comment', 'Language']]\n",
    "cleaned_columns = columns_to_clean.apply(remove_english_words)\n",
    "\n",
    "# Display the original and cleaned columns\n",
    "print(\"\\nCleaned Columns:\")\n",
    "print(cleaned_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0380ae40-32c7-4154-bc2e-b7182cc4515b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Sinhala comments DataFrame to a CSV file\n",
    "sinhala_comments_df[['Comment', 'Language']].to_csv('sinhala_comments.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273343ea-8d12-420f-94e1-77d30efd82a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
